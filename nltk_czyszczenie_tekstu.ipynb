{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFryG7CDs67o"
      },
      "source": [
        "# Czyszczenie tekstu przy pomocy bilbioteki NLTK\n",
        "Biblioteka Natural Language Toolkit (NLTK) to jedna z najpopularniejszych bibliotek do przetwarzania języka naturalnego w Pythonie. Jednym z kluczowych etapów w analizie tekstu jest czyszczenie danych, czyli usuwanie ze zbioru tekstu niepotrzebnych znaków, słów, interpunkcji itp. W tym nagłówku przedstawione zostaną podstawowe funkcjonalności biblioteki NLTK, które umożliwiają przeprowadzenie procesu czyszczenia tekstu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y3N4CWbnLZHs"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACb8B5VytvAU"
      },
      "source": [
        "**W jaki sposób można oczyścić tekst w wykorzystując NLTK?** <br>\n",
        "Biblioteka NLTK posiada dużą ilość tokenizatorów, które pozwalają na podzielenie tekstu na tokeny. Poza tą funkcjonalnością pozwalają również na usunięcie lub zachowanie określonych elementów w tekście, takich jak znaki interpunkcyjne czy numery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GAukPxvwcP5"
      },
      "source": [
        "**Przykładowe tokenizatory NLTK:** <br>\n",
        "\n",
        "\n",
        "*   Tweet Tokenizer\n",
        "*   Regular-Expression Tokenizers\n",
        "\n",
        "\n",
        "\n",
        "<br><br>\n",
        "*Pełna lista tokenizatorów: https://www.nltk.org/api/nltk.tokenize.html*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tFoH0EA6UME"
      },
      "source": [
        "# TweetTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN6hA56A6rJQ"
      },
      "source": [
        "Tokenizator zaprojektowany do tokenizacji tweetów. TweetTokenizer jest w stanie radzić sobie z trudnościami, które występują w analizie tweetów, takimi jak skróty, emotikony, hashtagi i adresy URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "46F30EdG6ICG"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tekst = \"@john: wwwwwwwww This is a cooool #dummysmiley: :-) :-P <3 +48333215665 https://www.nltk.org/ arrows < > -> <-- !!!!!!!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t094wva9moW"
      },
      "source": [
        "W celu oczyszczenia przetwarzanego tekstu Tweet Tokenizer wykorzystuje wyrażenia regularne. Parametry Tweet Tokenizer:\n",
        "\n",
        "\n",
        "*   **preserve_case** – odpowiada czy zachować wielkość liter przy tokenizacji. \n",
        "Domyślna wartość True\n",
        "*   **reduce_len** – odpowiada czy powtarzające się sekwencje o długości 3 lub więcej znaków mają być zastępowane sekwencją o długości 3 znaków. \n",
        "Domyśla wartość True\n",
        "*   **strip_handles** – odpowiada za usuwanie oznaczeń użytkowników (@nazwaUżytkownika). \n",
        "Domyślna wartość False\n",
        "*   **match_phone_numbers** – odpowiada za znajdywanie numerów telefonów w tekscie. \n",
        "Domyślna wartość True\n",
        "\n",
        "<br>\n",
        "Dla zainteresowanych: \n",
        "\n",
        "*   [kod źródłowy Tweet Tokenizer](https://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer.__init__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "37Hiu2yKBrKT"
      },
      "outputs": [],
      "source": [
        "tknzr1 = TweetTokenizer(preserve_case=False, reduce_len = True, strip_handles=True, match_phone_numbers=False)\n",
        "tknzr2 = TweetTokenizer()\n",
        "\n",
        "oczyszczony1 = tknzr1.tokenize(tekst)\n",
        "oczyszczony2 = tknzr2.tokenize(tekst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxj2LKsRCaYx"
      },
      "source": [
        "* Pierwszy tekst jest oryginalny i nie posiada żadnych zmian. \n",
        "*  W drugim tekscie widać, że zostało usunięte oznaczenie użytkownika, wszystkie duże litery zostały zmienione na małe litery, a sekwencja 9 liter 'w' została skrócona do trzech. \n",
        "* Trzeci tekst nie usuwa oznaczeń użytkowników jedynie wyszukuje tekście numery telefonów \n",
        "\n",
        "Warto również zauważyć że **reduce_len** automatycznie działa na znaki specjalne i nie ma możliwości wyłączenia tej funkcji z poziomu ustawiania parametru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ6MvwaL9m2_",
        "outputId": "6f777614-b59a-49be-bec5-1c0ac20dec36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1:\t@john: wwwwwwwww This is a cooool #dummysmiley: :-) :-P <3 +48333215665 https://www.nltk.org/ arrows < > -> <-- !!!!!!!\n",
            "2:\t: www this is a coool #dummysmiley : :-) :-P <3 + 48333215665 https://www.nltk.org/ arrows < > -> <-- ! ! !\n",
            "3:\t@john : wwwwwwwww This is a cooool #dummysmiley : :-) :-P <3 + 4833321566 5 https://www.nltk.org/ arrows < > -> <-- ! ! !\n"
          ]
        }
      ],
      "source": [
        "print(f\"1:\\t{tekst}\\n2:\\t{' '.join(oczyszczony1)}\\n3:\\t{' '.join(oczyszczony2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDShHheIJhmU"
      },
      "source": [
        "# Regular-Expression Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWnRSeoYJkPM"
      },
      "source": [
        "W odróżnieniu od innych tokenizerów, które używają zasad podziału na tokeny opartych na konwencjach językowych, tokenizatory oparte na wyrażeniach regularnych pozwalają na bardziej elastyczne dopasowywanie wzorców tokenów. Dzięki temu, wykorzystując tokenizatory oparte na wyrażeniach regularnych, możemy uzyskać dokładniejszą i bardziej spersonalizowaną tokenizację tekstu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dpLT9Y_cPcFx"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tekst = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\\n\\nNunadsa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO2ZWc7-SK_w",
        "outputId": "7de8cfea-4c97-4af4-ba6d-393b86cca130"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Good',\n",
              " 'muffins',\n",
              " 'cost',\n",
              " '$3.88',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York',\n",
              " '.',\n",
              " 'Please',\n",
              " 'buy',\n",
              " 'me',\n",
              " 'two',\n",
              " 'of',\n",
              " 'them',\n",
              " '.',\n",
              " 'Thanks',\n",
              " '.',\n",
              " 'Nunadsa']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tknzr = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+') \n",
        "tknzr.tokenize(tekst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFnxP4OuTkvW"
      },
      "source": [
        "Jak można zauwazyć RegexpTokenizer umożliwia zastosowanie spersonalizowanych wyrażeń regularnych. Wykorzystuje on wcześniej poznaną bilbiotekę re. Tokenizer ten umożliwa dostosowanie własnych parametrów takich jak:\n",
        "* pattern - spersonalizowane wyrażenie regularne\n",
        "* gaps - odpowiedzialny za znajdowanie separatorów pomiędzy tokenami, w przypadku ustawienia na fałsz spersonalizowane wyrażenie regularne powinno samo znajdować tokeny\n",
        "* discard_empty - odpowiedzialny za unikanie pustych tokenów ''. Mogą one być generowane tylko wtedy gdy wartość gaps == True\n",
        "* flags (int) - flagi regexp, które wykorzystuje się do kompilowania spersonalizowanego wyrażenia regularnego: re.UNICODE | re.MULTILINE | re.DOTALL\n",
        "<br>\n",
        "\n",
        "\n",
        "Dodatkowo klasa RegexpTokenizer posiada podklasy takie jak:\n",
        "* WhitespaceTokenizer\n",
        "* BlanklineTokenizer\n",
        "* WordPunctTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha_ArsChWko8"
      },
      "source": [
        "**WhitespaceTokenizer** <br>\n",
        "Podklasa klasy RegexpTokenizer, która przyjmuje następujące parametry klasy nadrzędnej:<br>\n",
        "**RegexpTokenizer(pattern=r\"\\s+\", gaps=True)**<br>\n",
        "Tokenizuje łańcuch tekstowy po białych znakach (spacja, tabulacja, nowa linia). Ogólnie rzecz biorąc, użytkownicy powinni zamiast tego używać metody split() dla stringów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS_hVRRHWpDU",
        "outputId": "a57c20d7-4f8a-497e-8f11-69a2cdb4519a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Good',\n",
              " 'muffins',\n",
              " 'cost',\n",
              " '$3.88',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York.',\n",
              " 'Please',\n",
              " 'buy',\n",
              " 'me',\n",
              " 'two',\n",
              " 'of',\n",
              " 'them.',\n",
              " 'Thanks.',\n",
              " 'Nunadsa']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "WhitespaceTokenizer().tokenize(tekst) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Dz_BbTXPD8"
      },
      "source": [
        "**WordPunctTokenizer** <br>\n",
        "Podklasa klasy RegexpTokenizer, która przyjmuje następujące parametry klasy nadrzędnej:<br>\n",
        "**RegexpTokenizer(r\"\\w+|[^\\w\\s]+\")**<br>\n",
        "Tokenizuje tekst na sekwencję znaków alfabetycznych i niealfabetycznych, używając wyrażeń regularnych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZo4k8wGYOnn",
        "outputId": "8a95cc07-66ca-4c90-9bc8-9e52c5541d6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Good',\n",
              " 'muffins',\n",
              " 'cost',\n",
              " '$',\n",
              " '3',\n",
              " '.',\n",
              " '88',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York',\n",
              " '.',\n",
              " 'Please',\n",
              " 'buy',\n",
              " 'me',\n",
              " 'two',\n",
              " 'of',\n",
              " 'them',\n",
              " '.',\n",
              " 'Thanks',\n",
              " '.',\n",
              " 'Nunadsa']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "WordPunctTokenizer().tokenize(tekst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbIgQIjfYr0o"
      },
      "source": [
        "**BlanklineTokenizer** <br>\n",
        "Podklasa klasy RegexpTokenizer, która przyjmuje następujące parametry klasy nadrzędnej:<br>\n",
        "**RegexpTokenizer(r\"\\s*\\n\\s*\\n\\s*\", gaps=True)**<br>\n",
        "Tokenizuje ciąg znaków, traktując dowolny ciąg pustych linii jako separator. Puste linie są definiowane jako linie nie zawierające znaków, z wyjątkiem spacji lub znaków tabulacji.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-hoq47MYsjT",
        "outputId": "a6795629-19b2-410b-a827-05923a217953"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.',\n",
              " 'Thanks.',\n",
              " 'Nunadsa']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import BlanklineTokenizer\n",
        "BlanklineTokenizer().tokenize(tekst)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NISTTokenizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`NISTTokenizer` to narzędzie do tokenizacji tekstu w języku angielskim, które jest dostępne w bibliotece NLTK. Tokenizacja odbywa się poprzez podział tekstu na pojedyncze słowa, liczby, symbole oraz znaki interpunkcyjne. `NISTTokenizer` jest oparty na regułach tokenizacji opracowanych przez Narodowy Instytut Standaryzacji i Technologii (The National Institute of Standards and Technology.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Good',\n",
              " 'muffins',\n",
              " 'cost',\n",
              " '$',\n",
              " '3.88',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York',\n",
              " '.',\n",
              " 'Please',\n",
              " 'buy',\n",
              " 'me',\n",
              " 'two',\n",
              " 'of',\n",
              " 'them',\n",
              " '.',\n",
              " 'Thanks',\n",
              " '.',\n",
              " 'Nunadsa']"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize.nist import NISTTokenizer\n",
        "NISTTokenizer().tokenize(tekst)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SyllableTokenizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`SyllableTokenizer` to klasa z pakietu `nltk.tokenize` w bibliotece Pythona NLTK, służąca do dzielenia tekstu na sylaby. Metoda `tokenize()` klasy `SyllableTokenizer` stosuje heurystyki, takie jak analiza sekwencji spółgłosek i samogłosek, aby podzielić słowa na sylaby. Istnieją jednak słowa, które nie da się podzielić na sylaby zgodnie z regułami fonetycznymi języka angielskiego, co może prowadzić do niepoprawnej tokenizacji.\n",
        "Używanie `SyllableTokenizer` może być przydatne, gdy chcemy wykonać analizę wiersza lub tekstu poetyckiego, gdzie sylaby są ważne z punktu widzenia rytmu i rymu. Jednakże, ze względu na ograniczenia w sposobie działania, zazwyczaj nie jest to idealne narzędzie do tokenizacji tekstu na poziomie słowa w języku angielskim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Bar', 'tlo', 'mie', 'j Br', 'zek']\n",
            "['Lu', 'kas', 'z ', 'Le', 'wic', 'ki']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import SyllableTokenizer\n",
        "print(SyllableTokenizer().tokenize('Bartlomiej Brzek'))\n",
        "print(SyllableTokenizer().tokenize('Lukasz Lewicki'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6tFoH0EA6UME"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
