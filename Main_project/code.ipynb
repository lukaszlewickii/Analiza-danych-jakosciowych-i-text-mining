{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Łukasz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora, models\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcja czytająca pliki w formacie PDF\n",
    "Funkcja `read_pdf` służy do odczytywania zawartości plików PDF. Przyjmuje jeden argument file_path, który jest ścieżką do pliku PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        text = ''\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1_path = 'J.R.R. Tolkien - Hobbit, czyli tam i z powrotem.pdf'\n",
    "book2_path = 'J.R.R Tolkien - Władca pierścieni. Bractwo Pierścienia. Tom 1.pdf'\n",
    "book3_path = 'J.R.R. Tolkien - Władca Pierścieni. Dwie Wieże. Tom 2.pdf'\n",
    "book4_path = 'J.R.R. Tolkien - Władca Pierścieni. Powrót Króla. Tom 3.pdf'\n",
    "\n",
    "book1 = read_pdf(book1_path)\n",
    "book2 = read_pdf(book2_path)\n",
    "book3 = read_pdf(book3_path)\n",
    "book4 = read_pdf(book4_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcja do czyszczenia i tokenizacji tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizacja tekstu\n",
    "\n",
    "def cleaned_text(text):\n",
    "    temp = re.sub(\"\\s{2,}\", \" \", text) #usunięcie podwójnych spacji\n",
    "    temp = re.sub(\"(\\r\\n|\\r|\\n)\", \" \", temp) #usunięcie przerw między słowami, entery\n",
    "    temp = temp.lower() #zamienia wszystkie znaki na małe litery\n",
    "    temp = re.sub(\"rt\", \"\", temp) #usunięcie retweetów\n",
    "    temp = re.sub(\"&amp\", \"\", temp) #usunięcie kodu htmlowego\n",
    "    temp = re.sub(\"#[a-z,A-Z]*\", \"\", temp) #usunięcie hasztagów\n",
    "    temp = re.sub(\"@\\w+\", \"\", temp) #usunięcie wzmianek, oznaczeń, tagowań\n",
    "    temp = re.sub(\"(f|ht)(tp)([^ ]*)\", \"\", temp) #usunięcie urlów\n",
    "    temp = re.sub(\"http(s?)([^ ]*)\", \"\", temp) #usunięcie urlów\n",
    "    temp = re.sub(\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\", \" \", temp) #usunięcie znaków specjalnych\n",
    "    temp = re.sub(\"\\d\", \"\", temp) #usunięcie cyfr\n",
    "    temp = re.sub(\"\\s{2,}\", \" \", temp) #ponowne usunięcie podwójnych spacji\n",
    "    temp = re.sub(\"[^\\w\\s]\", \"\", temp) #usunięcie znaków specjalnych\n",
    "    temp = temp.strip() #usunięcie spacji lub odstępów na końcach tekstu\n",
    "    \n",
    "    # Inicjalizacja stop words z biblioteki NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    my_stopwords = list(stop_words) + [str(letter) for letter in alphabet]\n",
    "    #tokenizacja tekstu za pomocą biblioteki NLTK\n",
    "    cleaned_and_tokenized = nltk.word_tokenize(temp)\n",
    "    \n",
    "    # Usunięcie stopwordsów\n",
    "    cleaned_and_tokenized_without_stopwords = [word for word in cleaned_and_tokenized if word.lower() not in my_stopwords]\n",
    "    \n",
    "    return cleaned_and_tokenized_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1_cleaned = cleaned_text(book1)\n",
    "book2_cleaned = cleaned_text(book2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Połączenie list unikalnych słów z obu dokumentów\n",
    "common_words = list(set(book1_cleaned).intersection(book2_cleaned))\n",
    "\n",
    "# Tworzenie chmury słów\n",
    "wordcloud = WordCloud(width=800, height=400).generate(' '.join(common_words))\n",
    "\n",
    "# Wyświetlenie chmury słów\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unikalne słowa występujące tylko w book1\n",
    "unique_words_book1 = list(set(book1_cleaned) - set(book2_cleaned))\n",
    "\n",
    "# Tworzenie chmury słów dla book1\n",
    "wordcloud_book1 = WordCloud(width=800, height=400).generate(' '.join(unique_words_book1))\n",
    "\n",
    "# Unikalne słowa występujące tylko w book2\n",
    "unique_words_book2 = list(set(book2_cleaned) - set(book1_cleaned))\n",
    "\n",
    "# Tworzenie chmury słów dla book2\n",
    "wordcloud_book2 = WordCloud(width=800, height=400).generate(' '.join(unique_words_book2))\n",
    "\n",
    "# Wyświetlenie chmur słów\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(wordcloud_book1, interpolation='bilinear')\n",
    "axes[0].set_title('Unique Words in Book 1')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(wordcloud_book2, interpolation='bilinear')\n",
    "axes[1].set_title('Unique Words in Book 2')\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porównanie książek w podziale na rozdziały"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chapter_from_pdf(book_path, start_page, end_page):\n",
    "    with open(book_path, 'rb') as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        text = ''\n",
    "        for page_number in range(start_page - 1, end_page):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text += page.extract_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyodrębnienie drugiego rozdziału z book1 (37-91 to strony, na których napisany jest rozdział 2 w PDF w book1)\n",
    "book1_chapter2 = extract_chapter_from_pdf('Noise a flaw in human judgement.pdf', 37, 91)\n",
    "\n",
    "# Wyodrębnienie pierwszego rozdziału z book2 (21-105 to strony na których napisany jest rozdział 1 w PDF w book2)\n",
    "book2_chapter1 = extract_chapter_from_pdf('Thinking fast and slow.pdf', 21, 105)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
